---
title: Unitary Evolution Recurrent Neural Networks
abstract: Recurrent neural networks (RNNs) are notoriously difficult to train. When
  the eigenvalues of the hidden to hidden weight matrix deviate from absolute value
  1, optimization becomes difficult due to the well studied issue of vanishing and
  exploding gradients, especially when trying to learn long-term dependencies. To
  circumvent this problem, we propose a new architecture that learns a unitary weight
  matrix, with eigenvalues of absolute value exactly 1. The challenge we address is
  that of parametrizing unitary matrices in a way that does not require expensive
  computations (such as eigendecomposition) after each weight update. We construct
  an expressive unitary weight matrix by composing several structured matrices that
  act as building blocks with parameters to be learned. Optimization with this parameterization
  becomes feasible only when considering hidden states in the complex domain. We demonstrate
  the potential of this architecture by achieving state of the art results in several
  hard tasks involving very long-term dependencies.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: arjovsky16
month: 0
tex_title: Unitary Evolution Recurrent Neural Networks
firstpage: 1120
lastpage: 1128
page: 1120-1128
sections: 
author:
- given: Martin
  family: Arjovsky
- given: Amar
  family: Shah
- given: Yoshua
  family: Bengio
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/arjovsky16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
