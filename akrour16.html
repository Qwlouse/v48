<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Model-Free Trajectory Optimization for Reinforcement Learning | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Model-Free Trajectory Optimization for Reinforcement Learning">

  <meta name="citation_author" content="Akrour, Riad">

  <meta name="citation_author" content="Neumann, Gerhard">

  <meta name="citation_author" content="Abdulsamad, Hany">

  <meta name="citation_author" content="Abdolmaleki, Abbas">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="2961">
<meta name="citation_lastpage" content="2970">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/akrour16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Model-Free Trajectory Optimization for Reinforcement Learning</h1>

	<div id="authors">
	
		Riad Akrour,
	
		Gerhard Neumann,
	
		Hany Abdulsamad,
	
		Abbas Abdolmaleki
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 2961â€“2970, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Many of the recent Trajectory Optimization algorithms alternate between local approximation of the dynamics and conservative policy update. However, linearly approximating the dynamics in order to derive the new policy can bias the update and prevent convergence to the optimal policy. In this article, we propose a new model-free algorithm that backpropagates a local quadratic time-dependent Q-Function, allowing the derivation of the policy update in closed form. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics demonstrating improved performance in comparison to related Trajectory Optimization algorithms linearizing the dynamics.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="akrour16.pdf">Download PDF</a></li>
			
			<li><a href="akrour16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
