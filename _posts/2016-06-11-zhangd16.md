---
title: L1-regularized Neural Networks are Improperly Learnable in Polynomial Time
abstract: We study the improper learning of multi-layer neural networks. Suppose that
  the neural network to be learned has k hidden layers and that the \ell_1-norm of
  the incoming weights of any neuron is bounded by L. We present a kernel-based method,
  such that with probability at least 1 - δ, it learns a predictor whose generalization
  error is at most εworse than that of the neural network. The sample complexity and
  the time complexity of the presented method are polynomial in the input dimension
  and in (1/ε,\log(1/δ),F(k,L)), where F(k,L) is a function depending on (k,L) and
  on the activation function, independent of the number of neurons. The algorithm
  applies to both sigmoid-like activation functions and ReLU-like activation functions.
  It implies that any sufficiently sparse neural network is learnable in polynomial
  time.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhangd16
month: 0
firstpage: 993
lastpage: 1001
page: 993-1001
sections: 
author:
- given: Yuchen
  family: Zhang
- given: Jason D.
  family: Lee
- given: Michael I.
  family: Jordan
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/zhangd16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
