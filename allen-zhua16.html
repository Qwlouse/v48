<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Variance Reduction for Faster Non-Convex Optimization | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Variance Reduction for Faster Non-Convex Optimization">

  <meta name="citation_author" content="Allen-Zhu, Zeyuan">

  <meta name="citation_author" content="Hazan, Elad">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="699">
<meta name="citation_lastpage" content="707">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/allen-zhua16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Variance Reduction for Faster Non-Convex Optimization</h1>

	<div id="authors">
	
		Zeyuan Allen-Zhu,
	
		Elad Hazan
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 699â€“707, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in <span class="math">\(O(1/\varepsilon)\)</span> iterations for smooth objectives, and stochastic gradient descent that converges in <span class="math">\(O(1/\varepsilon^2)\)</span> iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an <span class="math">\(O(1/\varepsilon)\)</span> rate, and is faster than full gradient descent by <span class="math">\(\Omega(n^{1/3})\)</span>. We demonstrate the effectiveness of our methods on empirical risk minimizations with non-convex loss functions and training neural nets.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="allen-zhua16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
