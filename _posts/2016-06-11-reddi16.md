---
supplementary: Supplementary:reddi16-supp.zip
title: Stochastic Variance Reduction for Nonconvex Optimization
abstract: We study nonconvex finite-sum problems and analyze stochastic variance reduced
  gradient (SVRG) methods for them. SVRG and related methods have recently surged
  into prominence for convex optimization given their edge over stochastic gradient
  descent (SGD); but their theoretical analysis almost exclusively assumes convexity.
  In contrast, we prove non-asymptotic rates of convergence (to stationary points)
  of SVRG for nonconvex optimization, and show that it is provably faster than SGD
  and gradient descent. We also analyze a subclass of nonconvex problems on which
  SVRG attains linear convergence to the global optimum. We extend our analysis to
  mini-batch variants of SVRG, showing (theoretical) linear speedup due to minibatching
  in parallel settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: reddi16
month: 0
firstpage: 314
lastpage: 323
page: 314-323
sections: 
author:
- given: Sashank J.
  family: Reddi
- given: Ahmed
  family: Hefny
- given: Suvrit
  family: Sra
- given: Barnabas
  family: Poczos
- given: Alex
  family: Smola
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/reddi16/reddi16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
