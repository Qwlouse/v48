---
supplementary: Supplementary:patrini16-supp.pdf
title: Loss factorization, weakly supervised learning and label noise robustness
abstract: We prove that the empirical risk of most well-known loss functions factors
  into a linear term aggregating all labels with a term that is label free, and can
  further be expressed by sums of the same loss. This holds true even for non-smooth,
  non-convex losses and in any RKHS. The first term is a (kernel) mean operator —
  the focal quantity of this work — which we characterize as the sufficient statistic
  for the labels. The result tightens known generalization bounds and sheds new light
  on their interpretation. Factorization has a direct application on weakly supervised
  learning. In particular, we demonstrate that algorithms like SGD and proximal methods
  can be adapted with minimal effort to handle weak supervision, once the mean operator
  has been estimated. We apply this idea to learning with asymmetric noisy labels,
  connecting and extending prior work. Furthermore, we show that most losses enjoy
  a data-dependent (by the mean operator) form of noise robustness, in contrast with
  known negative results.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: patrini16
month: 0
firstpage: 708
lastpage: 717
page: 708-717
sections: 
author:
- given: Giorgio
  family: Patrini
- given: Frank
  family: Nielsen
- given: Richard
  family: Nock
- given: Marcello
  family: Carioni
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/patrini16/patrini16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
