---
supplementary: Supplementary:songb16-supp.pdf
title: Training Deep Neural Networks via Direct Loss Minimization
abstract: Supervised training of deep neural nets typically relies on minimizing cross-entropy.
  However, in many domains, we are interested in performing well on metrics specific
  to the application. In this paper we propose a direct loss minimization approach
  to train deep neural networks, which provably minimizes the application-specific
  loss function. This is often non-trivial, since these functions are neither smooth
  nor decomposable and thus are not amenable to optimization with standard gradient-based
  methods. We demonstrate the effectiveness of our approach in the context of maximizing
  average precision for ranking problems. Towards this goal, we develop a novel dynamic
  programming algorithm that can efficiently compute the weight updates. Our approach
  proves superior to a variety of baselines in the context of action classification
  and object detection, especially in the presence of label noise.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: songb16
month: 0
tex_title: Training Deep Neural Networks via Direct Loss Minimization
firstpage: 2169
lastpage: 2177
page: 2169-2177
sections: 
author:
- given: Yang
  family: Song
- given: Alexander
  family: Schwing
- given: ''
  family: Richard
- given: Raquel
  family: Urtasun
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/songb16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
