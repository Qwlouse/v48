<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Learning and Inference via Maximum Inner Product Search | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Learning and Inference via Maximum Inner Product Search">

  <meta name="citation_author" content="Mussmann, Stephen">

  <meta name="citation_author" content="Ermon, Stefano">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="2587">
<meta name="citation_lastpage" content="2596">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/mussmann16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Learning and Inference via Maximum Inner Product Search</h1>

	<div id="authors">
	
		Stephen Mussmann,
	
		Stefano Ermon
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 2587â€“2596, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		A large class of commonly used probabilistic models known as log-linear models are defined up to a normalization constant.Typical learning algorithms for such models require solving a sequence of probabilistic inference queries. These inferences are typically intractable, and are a major bottleneck for learning models with large output spaces. In this paper, we provide a new approach for amortizing the cost of a sequence of related inference queries, such as the ones arising during learning. Our technique relies on a surprising connection with algorithms developed in the past two decades for similarity search in large data bases. Our approach achieves improved running times with provable approximation guarantees. We show that it performs well both on synthetic data and neural language models with large output spaces.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="mussmann16.pdf">Download PDF</a></li>
			
			<li><a href="mussmann16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
