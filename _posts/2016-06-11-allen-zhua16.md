---
title: Variance Reduction for Faster Non-Convex Optimization
abstract: We consider the fundamental problem in non-convex optimization of efficiently
  reaching a stationary point. In contrast to the convex case, in the long history
  of this basic problem, the only known theoretical results on first-order non-convex
  optimization remain to be full gradient descent that converges in O(1/\varepsilon)
  iterations for smooth objectives, and stochastic gradient descent that converges
  in O(1/\varepsilon^2) iterations for objectives that are sum of smooth functions.
  We provide the first improvement in this line of research. Our result is based on
  the variance reduction trick recently introduced to convex optimization, as well
  as a brand new analysis of variance reduction that is suitable for non-convex optimization.
  For objectives that are sum of smooth functions, our first-order minibatch stochastic
  method converges with an O(1/\varepsilon) rate, and is faster than full gradient
  descent by Î©(n^1/3). We demonstrate the effectiveness of our methods on empirical
  risk minimizations with non-convex loss functions and training neural nets.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: allen-zhua16
month: 0
firstpage: 699
lastpage: 707
page: 699-707
sections: 
author:
- given: Zeyuan
  family: Allen-Zhu
- given: Elad
  family: Hazan
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/allen-zhua16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
