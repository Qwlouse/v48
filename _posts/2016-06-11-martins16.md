---
supplementary: http://proceedings.mlr.press/v48/martins16-supp.pdf
title: 'From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification'
abstract: We propose sparsemax, a new activation function similar to the traditional
  softmax, but able to output sparse probabilities. After deriving its properties,
  we show how its Jacobian can be efficiently computed, enabling its use in a network
  trained with backpropagation. Then, we propose a new smooth and convex loss function
  which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection
  between this new loss and the Huber classification loss. We obtain promising empirical
  results in multi-label classification problems and in attention-based neural networks
  for natural language inference. For the latter, we achieve a similar performance
  as the traditional softmax, but with a selective, more compact, attention focus.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: martins16
month: 0
tex_title: 'From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label
  Classification'
firstpage: 1614
lastpage: 1623
page: 1614-1623
order: 1614
cycles: false
author:
- given: Andre
  family: Martins
- given: Ramon
  family: Astudillo
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/martins16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
