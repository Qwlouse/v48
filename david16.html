<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>PAC Lower Bounds and Efficient Algorithms for The Max <span class="math">\(K\)</span>-Armed Bandit Problem | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="PAC Lower Bounds and Efficient Algorithms for The Max $K$-Armed Bandit Problem">

  <meta name="citation_author" content="David, Yahel">

  <meta name="citation_author" content="Shimkin, Nahum">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="878">
<meta name="citation_lastpage" content="887">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/david16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>PAC Lower Bounds and Efficient Algorithms for The Max <span class="math">\(K\)</span>-Armed Bandit Problem</h1>

	<div id="authors">
	
		Yahel David,
	
		Nahum Shimkin
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 878â€“887, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the Max <span class="math">\(K\)</span>-Armed Bandit problem, where a learning agent is faced with several stochastic arms, each a source of i.i.d. rewards of unknown distribution. At each time step the agent chooses an arm, and observes the reward of the obtained sample. Each sample is considered here as a separate item with the reward designating its value, and the goal is to find an item with the highest possible value. Our basic assumption is a known lower bound on the <span><em>tail function</em></span> of the reward distributions. Under the PAC framework, we provide a lower bound on the sample complexity of any <span class="math">\((\epsilon,\delta)\)</span>-correct algorithm, and propose an algorithm that attains this bound up to logarithmic factors. We provide an analysis of the robustness of the proposed algorithm to the model assumptions, and further compare its performance to the simple non-adaptive variant, in which the arms are chosen randomly at each stage.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="david16.pdf">Download PDF</a></li>
			
			<li><a href="david16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
