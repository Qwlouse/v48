---
supplementary: Supplementary:jiang16-supp.pdf
title: Doubly Robust Off-policy Value Evaluation for Reinforcement Learning
abstract: 'We study the problem of off-policy value evaluation in reinforcement learning
  (RL), where one aims to estimate the value of a new policy based on data collected
  by a different policy. This problem is often a critical step when applying RL to
  real-world problems. Despite its importance, existing general methods either have
  uncontrolled bias or suffer high variance. In this work, we extend the doubly robust
  estimator for bandits to sequential decision-making problems, which gets the best
  of both worlds: it is guaranteed to be unbiased and can have a much lower variance
  than the popular importance sampling estimators. We demonstrate the estimatorâ€™s
  accuracy in several benchmark problems, and illustrate its use as a subroutine in
  safe policy improvement. We also provide theoretical results on the inherent hardness
  of the problem, and show that our estimator can match the lower bound in certain
  scenarios.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jiang16
month: 0
firstpage: 652
lastpage: 661
page: 652-661
sections: 
author:
- given: Nan
  family: Jiang
- given: Lihong
  family: Li
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/jiang16/jiang16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
