---
supplementary: Supplementary:safran16-supp.pdf
title: On the Quality of the Initial Basin in Overspecified Neural Networks
abstract: Deep learning, in the form of artificial neural networks, has achieved remarkable
  practical success in recent years, for a variety of difficult machine learning applications.
  However, a theoretical explanation for this remains a major open problem, since
  training neural networks involves optimizing a highly non-convex objective function,
  and is known to be computationally hard in the worst case. In this work, we study
  the \emphgeometric structure of the associated non-convex objective function, in
  the context of ReLU networks and starting from a random initialization of the network
  parameters. We identify some conditions under which it becomes more favorable to
  optimization, in the sense of (i) High probability of initializing at a point from
  which there is a monotonically decreasing path to a global minimum; and (ii) High
  probability of initializing at a basin (suitably defined) with a small minimal objective
  value. A common theme in our results is that such properties are more likely to
  hold for larger (“overspecified”) networks, which accords with some recent empirical
  and theoretical observations.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: safran16
month: 0
firstpage: 774
lastpage: 782
page: 774-782
sections: 
author:
- given: Itay
  family: Safran
- given: Ohad
  family: Shamir
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/safran16/safran16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
