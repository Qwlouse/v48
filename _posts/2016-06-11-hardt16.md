---
title: 'Train faster, generalize better: Stability of stochastic gradient descent'
abstract: We show that parametric models trained by a stochastic gradient method (SGM)
  with few iterations have vanishing generalization error. We prove our results by
  arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff.
  Our analysis only employs elementary tools from convex and continuous optimization.
  We derive stability bounds for both convex and non-convex optimization under standard
  Lipschitz and smoothness assumptions. Applying our results to the convex case, we
  provide new insights for why multiple epochs of stochastic gradient methods generalize
  well in practice. In the non-convex case, we give a new interpretation of common
  practices in neural networks, and formally show that popular techniques for training
  large deep models are indeed stability-promoting. Our findings conceptually underscore
  the importance of reducing training time beyond its obvious benefit.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hardt16
month: 0
tex_title: 'Train faster, generalize better: Stability of stochastic gradient descent'
firstpage: 1225
lastpage: 1234
page: 1225-1234
order: 1225
cycles: false
author:
- given: Moritz
  family: Hardt
- given: Ben
  family: Recht
- given: Yoram
  family: Singer
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/hardt16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
