<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Black-Box Alpha Divergence Minimization | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Black-Box Alpha Divergence Minimization">

  <meta name="citation_author" content="Hernandez-Lobato, Jose miguel">

  <meta name="citation_author" content="Li, Yingzhen">

  <meta name="citation_author" content="Rowland, Mark">

  <meta name="citation_author" content="Bui, Thang">

  <meta name="citation_author" content="Hernandez-Lobato, Daniel">

  <meta name="citation_author" content="Turner, Richard">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="1511">
<meta name="citation_lastpage" content="1520">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/hernandez-lobatob16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Black-Box Alpha Divergence Minimization</h1>

	<div id="authors">
	
		Jose miguel Hernandez-Lobato,
	
		Yingzhen Li,
	
		Mark Rowland,
	
		Thang Bui,
	
		Daniel Hernandez-Lobato,
	
		Richard Turner
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 1511â€“1520, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Black-box alpha (BB-<span class="math">\(\alpha\)</span>) is a new approximate inference method based on the minimization of <span class="math">\(\alpha\)</span>-divergences. BB-<span class="math">\(\alpha\)</span> scales to large datasets because it can be implemented using stochastic gradient descent. BB-<span class="math">\(\alpha\)</span> can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter <span class="math">\(\alpha\)</span>, the method is able to interpolate between variational Bayes (VB) (<span class="math">\(\alpha \rightarrow 0\)</span>) and an algorithm similar to expectation propagation (EP) (<span class="math">\(\alpha = 1\)</span>). Experiments on probit regression and neural network regression and classification problems show that BB-<span class="math">\(\alpha\)</span> with non-standard settings of <span class="math">\(\alpha\)</span>, such as <span class="math">\(\alpha = 0.5\)</span>, usually produces better predictions than with <span class="math">\(\alpha \rightarrow 0\)</span> (VB) or <span class="math">\(\alpha = 1\)</span> (EP).
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="hernandez-lobatob16.pdf">Download PDF</a></li>
			
			<li><a href="hernandez-lobatob16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
