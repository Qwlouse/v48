---
supplementary: Supplementary:louizos16-supp.pdf
title: Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors
abstract: We introduce a variational Bayesian neural network where the parameters
  are governed via a probability distribution on random matrices. Specifically, we
  employ a matrix variate Gaussian (Gupta & Nagar ’99) parameter posterior distribution
  where we explicitly model the covariance among the input and output dimensions of
  each layer. Furthermore, with approximate covariance matrices we can achieve a more
  efficient way to represent those correlations that is also cheaper than fully factorized
  parameter posteriors. We further show that with the “local reprarametrization trick"
  (Kingma & Welling ’15) on this posterior distribution we arrive at a Gaussian Process
  (Rasmussen ’06) interpretation of the hidden units in each layer and we, similarly
  with (Gal & Ghahramani ’15), provide connections with deep Gaussian processes. We
  continue in taking advantage of this duality and incorporate “pseudo-data” (Snelson
  & Ghahramani ’05) in our model, which in turn allows for more efficient posterior
  sampling while maintaining the properties of the original model. The validity of
  the proposed approach is verified through extensive experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: louizos16
month: 0
firstpage: 1708
lastpage: 1716
page: 1708-1716
sections: 
author:
- given: Christos
  family: Louizos
- given: Max
  family: Welling
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/louizos16/louizos16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
