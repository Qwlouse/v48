---
supplementary: Supplementary:zhangb16-supp.pdf
title: Online Stochastic Linear Optimization under One-bit Feedback
abstract: In this paper, we study a special bandit setting of online stochastic linear
  optimization, where only one-bit of information is revealed to the learner at each
  round. This problem has found many applications including online advertisement and
  online recommendation. We assume the binary feedback is a random variable generated
  from the logit model, and aim to minimize the regret defined by the unknown linear
  function. Although the existing method for generalized linear bandit can be applied
  to our problem, the high computational cost makes it impractical for real-world
  applications. To address this challenge, we develop an efficient online learning
  algorithm by exploiting particular structures of the observation model. Specifically,
  we adopt online Newton step to estimate the unknown parameter and derive a tight
  confidence region based on the exponential concavity of the logistic loss. Our analysis
  shows that the proposed algorithm achieves a regret bound of O(d\sqrtT), which matches
  the optimal result of stochastic linear bandits.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhangb16
month: 0
firstpage: 392
lastpage: 401
page: 392-401
sections: 
author:
- given: Lijun
  family: Zhang
- given: Tianbao
  family: Yang
- given: Rong
  family: Jin
- given: Yichi
  family: Xiao
- given: Zhi-hua
  family: Zhou
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/zhangb16/zhangb16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
