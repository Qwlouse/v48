---
title: Large-Margin Softmax Loss for Convolutional Neural Networks
abstract: Cross-entropy loss together with softmax is arguably one of the most common
  used supervision components in convolutional neural networks (CNNs). Despite its
  simplicity, popularity and excellent performance, the component does not explicitly
  encourage discriminative learning of features. In this paper, we propose a generalized
  large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness
  and inter-class separability between learned features. Moreover, L-Softmax not only
  can adjust the desired margin but also can avoid overfitting. We also show that
  the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive
  experiments on four benchmark datasets demonstrate that the deeply-learned features
  with L-softmax loss become more discriminative, hence significantly boosting the
  performance on a variety of visual classification and verification tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: liud16
month: 0
tex_title: Large-Margin Softmax Loss for Convolutional Neural Networks
firstpage: 507
lastpage: 516
page: 507-516
order: 507
cycles: false
author:
- given: Weiyang
  family: Liu
- given: Yandong
  family: Wen
- given: Zhiding
  family: Yu
- given: Meng
  family: Yang
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/liud16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
