---
title: Adaptive Sampling for SGD by Exploiting Side Information
abstract: This paper proposes a new mechanism for sampling training instances for
  stochastic gradient descent (SGD) methods by exploiting any side-information associated
  with the instances (for e.g. class-labels) to improve convergence. Previous methods
  have either relied on sampling from a distribution defined over training instances
  or from a static distribution that fixed before training. This results in two problems
  a) any distribution that is set apriori is independent of how the optimization progresses
  and b) maintaining a distribution over individual instances could be infeasible
  in large-scale scenarios. In this paper, we exploit the side information associated
  with the instances to tackle both problems. More specifically, we maintain a distribution
  over classes (instead of individual instances) that is adaptively estimated during
  the course of optimization to give the maximum reduction in the variance of the
  gradient. Intuitively, we sample more from those regions in space that have a \textitlarger
  gradient contribution. Our experiments on highly multiclass datasets show that our
  proposal converge significantly faster than existing techniques.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gopal16
month: 0
tex_title: Adaptive Sampling for SGD by Exploiting Side Information
firstpage: 364
lastpage: 372
page: 364-372
sections: 
author:
- given: Siddharth
  family: Gopal
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/gopal16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
