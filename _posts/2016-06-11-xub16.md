---
supplementary: Supplementary:xub16-supp.pdf
title: Matrix Eigen-decomposition via Doubly Stochastic Riemannian Optimization
abstract: Matrix eigen-decomposition is a classic and long-standing problem that plays
  a fundamental role in scientific computing and machine learning. Despite some existing
  algorithms for this inherently non-convex problem, the study remains inadequate
  for the need of large data nowadays. To address this gap, we propose a Doubly Stochastic
  Riemannian Gradient EIGenSolver, DSRG-EIGS, where the double stochasticity comes
  from the generalization of the stochastic Euclidean gradient ascent and the stochastic
  Euclidean coordinate ascent to Riemannian manifolds. As a result, it induces a greatly
  reduced complexity per iteration, enables the algorithm to completely avoid the
  matrix inversion, and consequently makes it well-suited to large-scale applications.
  We theoretically analyze its convergence properties and empirically validate it
  on real-world datasets. Encouraging experimental results demonstrate its advantages
  over the deterministic counterparts.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: xub16
month: 0
tex_title: Matrix Eigen-decomposition via Doubly Stochastic Riemannian Optimization
firstpage: 1660
lastpage: 1669
page: 1660-1669
sections: 
author:
- given: Zhiqiang
  family: Xu
- given: Peilin
  family: Zhao
- given: Jianneng
  family: Cao
- given: Xiaoli
  family: Li
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/xub16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
