---
title: Multi-Bias Non-linear Activation in Deep Neural Networks
abstract: As a widely used non-linear activation, Rectified Linear Unit (ReLU) separates
  noise and signal in a feature map by learning a threshold or bias. However, we argue
  that the classification of noise and signal not only depends on the magnitude of
  responses, but also the context of how the feature responses would be used to detect
  more abstract patterns in higher layers. In order to output multiple response maps
  with magnitude in different ranges for a particular visual pattern, existing networks
  employing ReLU and its variants have to learn a large number of redundant filters.
  In this paper, we propose a multi-bias non-linear activation (MBA) layer to explore
  the information hidden in the magnitudes of responses. It is placed after the convolution
  layer to decouple the responses to a convolution kernel into multiple maps by multi-thresholding
  magnitudes, thus generating more patterns in the feature space at a low computational
  cost. It provides great flexibility of selecting responses to different visual patterns
  in different magnitude ranges to form rich representations in higher layers. Such
  a simple and yet effective scheme achieves the state-of-the-art performance on several
  benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lia16
month: 0
tex_title: Multi-Bias Non-linear Activation in Deep Neural Networks
firstpage: 221
lastpage: 229
page: 221-229
sections: 
author:
- given: Hongyang
  family: Li
- given: Wanli
  family: Ouyang
- given: Xiaogang
  family: Wang
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/lia16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
