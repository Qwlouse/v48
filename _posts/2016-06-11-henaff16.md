---
title: Recurrent Orthogonal Networks and Long-Memory Tasks
abstract: Although RNNs have been shown to be power- ful tools for processing sequential
  data, finding architectures or optimization strategies that al- low them to model
  very long term dependencies is still an active area of research. In this work, we
  carefully analyze two synthetic datasets orig- inally outlined in (Hochreiter &
  Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information
  over many time steps. We explicitly construct RNN solutions to these problems, and
  using these constructions, illumi- nate both the problems themselves and the way
  in which RNNs store different types of information in their hidden states. These
  constructions fur- thermore explain the success of recent methods that specify unitary
  initializations or constraints on the transition matrices.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: henaff16
month: 0
tex_title: Recurrent Orthogonal Networks and Long-Memory Tasks
firstpage: 2034
lastpage: 2042
page: 2034-2042
sections: 
author:
- given: Mikael
  family: Henaff
- given: Arthur
  family: Szlam
- given: Yann
  family: LeCun
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/henaff16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
