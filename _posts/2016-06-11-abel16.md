---
title: Near Optimal Behavior via Approximate State Abstraction
abstract: The combinatorial explosion that plagues planning and reinforcement learning
  (RL) algorithms can be moderated using state abstraction. Prohibitively large task
  representations can be condensed such that essential information is preserved, and
  consequently, solutions are tractably computable. However, exact abstractions, which
  treat only fully-identical situations as equivalent, fail to present opportunities
  for abstraction in environments where no two situations are exactly alike. In this
  work, we investigate approximate state abstractions, which treat nearly-identical
  situations as equivalent. We present theoretical guarantees of the quality of behaviors
  derived from four types of approximate abstractions. Additionally, we empirically
  demonstrate that approximate abstractions lead to reduction in task complexity and
  bounded loss of optimality of behavior in a variety of environments.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: abel16
month: 0
tex_title: Near Optimal Behavior via Approximate State Abstraction
firstpage: 2915
lastpage: 2923
page: 2915-2923
sections: 
author:
- given: David
  family: Abel
- given: David
  family: Hershkowitz
- given: Michael
  family: Littman
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/abel16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
