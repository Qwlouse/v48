<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>How to Fake Multiply by a Gaussian Matrix | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="How to Fake Multiply by a Gaussian Matrix">

  <meta name="citation_author" content="Kapralov, Michael">

  <meta name="citation_author" content="Potluru, Vamsi">

  <meta name="citation_author" content="Woodruff, David">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="2101">
<meta name="citation_lastpage" content="2110">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/kapralov16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>How to Fake Multiply by a Gaussian Matrix</h1>

	<div id="authors">
	
		Michael Kapralov,
	
		Vamsi Potluru,
	
		David Woodruff
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 2101–2110, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Have you ever wanted to multiply an <span class="math">\(n \times d\)</span> matrix <span class="math">\(X\)</span>, with <span class="math">\(n \gg d\)</span>, on the left by an <span class="math">\(m \times n\)</span> matrix <span class="math">\(\tilde G\)</span> of i.i.d. Gaussian random variables, but could not afford to do it because it was too slow? In this work we propose a new randomized <span class="math">\(m \times n\)</span> matrix <span class="math">\(T\)</span>, for which one can compute <span class="math">\(T \cdot X\)</span> in only <span class="math">\(O(nnz(X)) + \tilde O(m^{1.5} \cdot d^{3})\)</span> time, for which the total variation distance between the distributions <span class="math">\(T \cdot X\)</span> and <span class="math">\(\tilde G \cdot X\)</span> is as small as desired, i.e., less than any positive constant. Here <span class="math">\(nnz(X)\)</span> denotes the number of non-zero entries of <span class="math">\(X\)</span>. Assuming <span class="math">\(nnz(X) \gg m^{1.5} \cdot d^{3}\)</span>, this is a significant savings over the naïve <span class="math">\(O(nnz(X) m)\)</span> time to compute <span class="math">\(\tilde G \cdot X\)</span>. Moreover, since the total variation distance is small, we can provably use <span class="math">\(T \cdot X\)</span> in place of <span class="math">\(\tilde G \cdot X\)</span> in any application and have the same guarantees as if we were using <span class="math">\(\tilde G \cdot X\)</span>, up to a small positive constant in error probability. We apply this transform to nonnegative matrix factorization (NMF) and support vector machines (SVM).
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="kapralov16.pdf">Download PDF</a></li>
			
			<li><a href="kapralov16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
