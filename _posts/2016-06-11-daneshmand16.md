---
supplementary: Supplementary:daneshmand16-supp.pdf
title: Starting Small - Learning with Adaptive Sample Sizes
abstract: For many machine learning problems, data is abundant and it may be prohibitive
  to make multiple passes through the full training set. In this context, we investigate
  strategies for dynamically increasing the effective sample size, when using iterative
  methods such as stochastic gradient descent. Our interest is motivated by the rise
  of variance-reduced methods, which achieve linear convergence rates that scale favorably
  for smaller sample sizes. Exploiting this feature, we show - theoretically and empirically
  - how to obtain significant speed-ups with a novel algorithm that reaches statistical
  accuracy on an n-sample in 2n, instead of n log n steps.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: daneshmand16
month: 0
tex_title: Starting Small - Learning with Adaptive Sample Sizes
firstpage: 1463
lastpage: 1471
page: 1463-1471
sections: 
author:
- given: Hadi
  family: Daneshmand
- given: Aurelien
  family: Lucchi
- given: Thomas
  family: Hofmann
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/daneshmand16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
