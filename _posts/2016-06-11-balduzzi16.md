---
title: Strongly-Typed Recurrent Neural Networks
abstract: Recurrent neural networks are increasing popular models for sequential learning.
  Unfortunately, although the most effective RNN architectures are perhaps excessively
  complicated, extensive searches have not found simpler alternatives. This paper
  imports ideas from physics and functional programming into RNN design to provide
  guiding principles. From physics, we introduce type constraints, analogous to the
  constraints that forbids adding meters to seconds. From functional programming,
  we require that strongly-typed architectures factorize into stateless learnware
  and state-dependent firmware, reducing the impact of side-effects. The features
  learned by strongly-typed nets have a simple semantic interpretation via dynamic
  average-pooling on one-dimensional convolutions. We also show that strongly-typed
  gradients are better behaved than in classical architectures, and characterize the
  representational power of strongly-typed nets. Finally, experiments show that, despite
  being more constrained, strongly-typed architectures achieve lower training and
  comparable generalization error to classical architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: balduzzi16
month: 0
firstpage: 1292
lastpage: 1300
page: 1292-1300
sections: 
author:
- given: David
  family: Balduzzi
- given: Muhammad
  family: Ghifary
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/balduzzi16/balduzzi16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
