---
supplementary: http://proceedings.mlr.press/v48/arpita16-supp.pdf
title: Why Regularized Auto-Encoders learn Sparse Representation?
abstract: Sparse distributed representation is the key to learning useful features
  in deep learning algorithms, because not only it is an efficient mode of data representation,
  but also – more importantly – it captures the generation process of most real world
  data. While a number of regularized auto-encoders (AE) enforce sparsity explicitly
  in their learned representation and others don’t, there has been little formal analysis
  on what encourages sparsity in these models in general. Our objective is to formally
  study this general problem for regularized auto-encoders. We provide sufficient
  conditions on both regularization and activation functions that encourage sparsity.
  We show that multiple popular models (de-noising and contractive auto encoders,
  e.g.) and activations (rectified linear and sigmoid, e.g.) satisfy these conditions;
  thus, our conditions help explain sparsity in their learned representation. Thus
  our theoretical and empirical analysis together shed light on the properties of
  regularization/activation that are conductive to sparsity and unify a number of
  existing auto-encoder models and activation functions under the same analytical
  framework.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: arpita16
month: 0
tex_title: Why Regularized Auto-Encoders learn Sparse Representation?
firstpage: 136
lastpage: 144
page: 136-144
order: 136
cycles: false
author:
- given: Devansh
  family: Arpit
- given: Yingbo
  family: Zhou
- given: Hung
  family: Ngo
- given: Venu
  family: Govindaraju
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/arpita16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
