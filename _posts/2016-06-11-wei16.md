---
title: Network Morphism
abstract: We present a systematic study on how to morph a well-trained neural network
  to a new one so that its network function can be completely preserved. We define
  this as network morphism in this research. After morphing a parent network, the
  child network is expected to inherit the knowledge from its parent network and also
  has the potential to continue growing into a more powerful one with much shortened
  training time. The first requirement for this network morphism is its ability to
  handle diverse morphing types of networks, including changes of depth, width, kernel
  size, and even subnet. To meet this requirement, we first introduce the network
  morphism equations, and then develop novel morphing algorithms for all these morphing
  types for both classic and convolutional neural networks. The second requirement
  is its ability to deal with non-linearity in a network. We propose a family of parametric-activation
  functions to facilitate the morphing of any continuous non-linear activation neurons.
  Experimental results on benchmark datasets and typical neural networks demonstrate
  the effectiveness of the proposed network morphism scheme.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wei16
month: 0
firstpage: 564
lastpage: 572
page: 564-572
sections: 
author:
- given: Tao
  family: Wei
- given: Changhu
  family: Wang
- given: Yong
  family: Rui
- given: Chang Wen
  family: Chen
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/wei16/wei16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
