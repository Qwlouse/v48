---
title: Efficient Learning with a Family of Nonconvex Regularizers by Redistributing
  Nonconvexity
abstract: The use of convex regularizers allow for easy optimization, though they
  often produce biased estimation and inferior prediction performance. Recently, nonconvex
  regularizers have attracted a lot of attention and outperformed convex ones. However,
  the resultant optimization problem is much harder. In this paper, for a large class
  of nonconvex regularizers, we propose to move the nonconvexity from the regularizer
  to the loss. The nonconvex regularizer is then transformed to a familiar convex
  regularizer, while the resultant loss function can still be guaranteed to be smooth.
  Learning with the convexified regularizer can be performed by existing efficient
  algorithms originally designed for convex regularizers (such as the standard proximal
  algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points
  of the transformed problem are also critical points of the original problem. Extensive
  experiments on a number of nonconvex regularization problems show that the proposed
  procedure is much faster than the state-of-the-art nonconvex solvers.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yao16
month: 0
tex_title: Efficient Learning with a Family of Nonconvex Regularizers by Redistributing
  Nonconvexity
firstpage: 2645
lastpage: 2654
page: 2645-2654
order: 2645
cycles: false
author:
- given: Quanming
  family: Yao
- given: James
  family: Kwok
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/yao16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
