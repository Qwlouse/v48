---
title: A Convolutional Attention Network for Extreme Summarization of Source Code
abstract: 'Attention mechanisms in neural networks have proved useful for problems
  in which the input and output do not have fixed dimension. Often there exist features
  that are locally translation invariant and would be valuable for directing the model’s
  attention, but previous attentional architectures are not constructed to learn such
  features specifically. We introduce an attentional neural network that employs convolution
  on the input tokens to detect local time-invariant and long-range topical attention
  features in a context-dependent way. We apply this architecture to the problem of
  extreme summarization of source code snippets into short, descriptive function name-like
  summaries. Using those features, the model sequentially generates a summary by marginalizing
  over two attention mechanisms: one that predicts the next summary token based on
  the attention weights of the input tokens and another that is able to copy a code
  token as-is directly into the summary. We demonstrate our convolutional attention
  neural network’s performance on 10 popular Java projects showing that it achieves
  better performance compared to previous attentional mechanisms.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: allamanis16
month: 0
firstpage: 2091
lastpage: 2100
page: 2091-2100
sections: 
author:
- given: Miltiadis
  family: Allamanis
- given: Hao
  family: Peng
- given: Charles
  family: Sutton
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/allamanis16/allamanis16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
