<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Exploiting Cyclic Symmetry in Convolutional Neural Networks | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Exploiting Cyclic Symmetry in Convolutional Neural Networks">

  <meta name="citation_author" content="Dieleman, Sander">

  <meta name="citation_author" content="Fauw, Jeffrey De">

  <meta name="citation_author" content="Kavukcuoglu, Koray">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="1889">
<meta name="citation_lastpage" content="1898">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/dieleman16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Exploiting Cyclic Symmetry in Convolutional Neural Networks</h1>

	<div id="authors">
	
		Sander Dieleman,
	
		Jeffrey De Fauw,
	
		Koray Kavukcuoglu
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 1889â€“1898, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="dieleman16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
