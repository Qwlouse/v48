<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Minimizing the Maximal Loss: How and Why | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Minimizing the Maximal Loss: How and Why">

  <meta name="citation_author" content="Shalev-Shwartz, Shai">

  <meta name="citation_author" content="Wexler, Yonatan">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="793">
<meta name="citation_lastpage" content="801">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/shalev-shwartzb16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Minimizing the Maximal Loss: How and Why</h1>

	<div id="authors">
	
		Shai Shalev-Shwartz,
	
		Yonatan Wexler
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 793â€“801, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		A commonly used learning rule is to approximately minimize the <em>average</em> loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the <em>maximal</em> loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We show, theoretically and empirically, that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="shalev-shwartzb16.pdf">Download PDF</a></li>
			
			<li><a href="shalev-shwartzb16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
