<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient">

  <meta name="citation_author" content="Yang, Tianbao">

  <meta name="citation_author" content="Zhang, Lijun">

  <meta name="citation_author" content="Jin, Rong">

  <meta name="citation_author" content="Yi, Jinfeng">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="449">
<meta name="citation_lastpage" content="457">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/yangb16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient</h1>

	<div id="authors">
	
		Tianbao Yang,
	
		Lijun Zhang,
	
		Rong Jin,
	
		Jinfeng Yi
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 449–457, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variation-based upper bounds of the dynamic regret under the true and noisy gradient feedback, which are <span><em>optimal</em></span> in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant’s minimizers, to which we refer as path variation. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches that is achieved with full information.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="yangb16.pdf">Download PDF</a></li>
			
			<li><a href="yangb16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
