---
supplementary: Supplementary:akrour16-supp.pdf
title: Model-Free Trajectory Optimization for Reinforcement Learning
abstract: Many of the recent Trajectory Optimization algorithms alternate between
  local approximation of the dynamics and conservative policy update. However, linearly
  approximating the dynamics in order to derive the new policy can bias the update
  and prevent convergence to the optimal policy. In this article, we propose a new
  model-free algorithm that backpropagates a local quadratic time-dependent Q-Function,
  allowing the derivation of the policy update in closed form. Our policy update ensures
  exact KL-constraint satisfaction without simplifying assumptions on the system dynamics
  demonstrating improved performance in comparison to related Trajectory Optimization
  algorithms linearizing the dynamics.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: akrour16
month: 0
tex_title: Model-Free Trajectory Optimization for Reinforcement Learning
firstpage: 2961
lastpage: 2970
page: 2961-2970
sections: 
author:
- given: Riad
  family: Akrour
- given: Gerhard
  family: Neumann
- given: Hany
  family: Abdulsamad
- given: Abbas
  family: Abdolmaleki
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/akrour16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
