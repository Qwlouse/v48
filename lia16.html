<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Multi-Bias Non-linear Activation in Deep Neural Networks | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Multi-Bias Non-linear Activation in Deep Neural Networks">

  <meta name="citation_author" content="Li, Hongyang">

  <meta name="citation_author" content="Ouyang, Wanli">

  <meta name="citation_author" content="Wang, Xiaogang">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="221">
<meta name="citation_lastpage" content="229">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/lia16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Multi-Bias Non-linear Activation in Deep Neural Networks</h1>

	<div id="authors">
	
		Hongyang Li,
	
		Wanli Ouyang,
	
		Xiaogang Wang
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 221â€“229, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		As a widely used non-linear activation, Rectified Linear Unit (ReLU) separates noise and signal in a feature map by learning a threshold or bias. However, we argue that the classification of noise and signal not only depends on the magnitude of responses, but also the context of how the feature responses would be used to detect more abstract patterns in higher layers. In order to output multiple response maps with magnitude in different ranges for a particular visual pattern, existing networks employing ReLU and its variants have to learn a large number of redundant filters. In this paper, we propose a multi-bias non-linear activation (MBA) layer to explore the information hidden in the magnitudes of responses. It is placed after the convolution layer to decouple the responses to a convolution kernel into multiple maps by multi-thresholding magnitudes, thus generating more patterns in the feature space at a low computational cost. It provides great flexibility of selecting responses to different visual patterns in different magnitude ranges to form rich representations in higher layers. Such a simple and yet effective scheme achieves the state-of-the-art performance on several benchmarks.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="lia16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
