---
supplementary: Supplementary:santoro16-supp.pdf
title: Meta-Learning with Memory-Augmented Neural Networks
abstract: Despite recent breakthroughs in the applications of deep neural networks,
  one setting that presents a persistent challenge is that of "one-shot learning."
  Traditional gradient-based networks require a lot of data to learn, often through
  extensive iterative training. When new data is encountered, the models must inefficiently
  relearn their parameters to adequately incorporate the new information without catastrophic
  interference. Architectures with augmented memory capacities, such as Neural Turing
  Machines (NTMs), offer the ability to quickly encode and retrieve new information,
  and hence can potentially obviate the downsides of conventional models. Here, we
  demonstrate the ability of a memory-augmented neural network to rapidly assimilate
  new data, and leverage this data to make accurate predictions after only a few samples.
  We also introduce a new method for accessing an external memory that focuses on
  memory content, unlike previous methods that additionally use memory location-based
  focusing mechanisms.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: santoro16
month: 0
firstpage: 1842
lastpage: 1850
page: 1842-1850
sections: 
author:
- given: Adam
  family: Santoro
- given: Sergey
  family: Bartunov
- given: Matthew
  family: Botvinick
- given: Daan
  family: Wierstra
- given: Timothy
  family: Lillicrap
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/santoro16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
