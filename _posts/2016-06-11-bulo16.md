---
supplementary: Supplementary:bulo16-supp.pdf
title: Dropout distillation
abstract: Dropout is a popular stochastic regularization technique for deep neural
  networks that works by randomly dropping (i.e. zeroing) units from the network during
  training. This randomization process allows to implicitly train an ensemble of exponentially
  many networks sharing the same parametrization, which should be averaged at test
  time to deliver the final prediction. A typical workaround for this intractable
  averaging operation consists in scaling the layers undergoing dropout randomization.
  This simple rule called ’standard dropout’ is efficient, but might degrade the accuracy
  of the prediction. In this work we introduce a novel approach, coined ’dropout distillation’,
  that allows us to train a predictor in a way to better approximate the intractable,
  but preferable, averaging process, while keeping under control its computational
  efficiency. We are thus able to construct models that are as efficient as standard
  dropout, or even more efficient, while being more accurate. Experiments on standard
  benchmark datasets demonstrate the validity of our method, yielding consistent improvements
  over conventional dropout.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bulo16
month: 0
firstpage: 99
lastpage: 107
page: 99-107
sections: 
author:
- given: Samuel Rota
  family: Bulò
- given: Lorenzo
  family: Porzi
- given: Peter
  family: Kontschieder
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/bulo16/bulo16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
