---
supplementary: Supplementary:shamirb16-supp.pdf
title: Convergence of Stochastic Gradient Descent for PCA
abstract: We consider the problem of principal component analysis (PCA) in a streaming
  stochastic setting, where our goal is to find a direction of approximate maximal
  variance, based on a stream of i.i.d. data points in R^d. A simple and computationally
  cheap algorithm for this is stochastic gradient descent (SGD), which incrementally
  updates its estimate based on each new data point. However, due to the non-convex
  nature of the problem, analyzing its performance has been a challenge. In particular,
  existing guarantees rely on a non-trivial eigengap assumption on the covariance
  matrix, which is intuitively unnecessary. In this paper, we provide (to the best
  of our knowledge) the first eigengap-free convergence guarantees for SGD in the
  context of PCA. This also partially resolves an open problem posed in (Hardt & Price,
  2014). Moreover, under an eigengap assumption, we show that the same techniques
  lead to new SGD convergence guarantees with better dependence on the eigengap.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shamirb16
month: 0
firstpage: 257
lastpage: 265
page: 257-265
sections: 
author:
- given: Ohad
  family: Shamir
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/shamirb16/shamirb16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
