---
title: Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling
abstract: Accelerated coordinate descent is widely used in optimization due to its
  cheap per-iteration cost and scalability to large-scale problems. Up to a primal-dual
  transformation, it is also the same as accelerated stochastic gradient descent that
  is one of the central methods used in machine learning. In this paper, we improve
  the best known running time of accelerated coordinate descent by a factor up to
  \sqrtn. Our improvement is based on a clean, novel non-uniform sampling that selects
  each coordinate with a probability proportional to the square root of its smoothness
  parameter. Our proof technique also deviates from the classical estimation sequence
  technique used in prior work. Our speed-up applies to important problems such as
  empirical risk minimization and solving linear systems, both in theory and in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: allen-zhuc16
month: 0
tex_title: Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling
firstpage: 1110
lastpage: 1119
page: 1110-1119
sections: 
author:
- given: Zeyuan
  family: Allen-Zhu
- given: Zheng
  family: Qu
- given: Peter
  family: Richtarik
- given: Yang
  family: Yuan
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/allen-zhuc16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
