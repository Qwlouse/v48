---
title: 'Training Neural Networks Without Gradients: A Scalable ADMM Approach'
abstract: With the growing importance of large network models and enormous training
  datasets, GPUs have become increasingly necessary to train neural networks. This
  is largely because conventional optimization algorithms rely on stochastic gradient
  methods that donâ€™t scale well to large numbers of cores in a cluster setting. Furthermore,
  the convergence of all gradient methods, including batch methods, suffers from common
  problems like saturation effects, poor conditioning, and saddle points. This paper
  explores an unconventional training method that uses alternating direction methods
  and Bregman iteration to train networks without gradient descent steps. The proposed
  method reduces the network training problem to a sequence of minimization sub-steps
  that can each be solved globally in closed form. The proposed method is advantageous
  because it avoids many of the caveats that make gradient methods slow on highly
  non-convex problems. In addition, the method exhibits strong scaling in the distributed
  setting, yielding linear speedups even when split over thousands of cores.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: taylor16
month: 0
firstpage: 2722
lastpage: 2731
page: 2722-2731
sections: 
author:
- given: Gavin
  family: Taylor
- given: Ryan
  family: Burmeister
- given: Zheng
  family: Xu
- given: Bharat
  family: Singh
- given: Ankit
  family: Patel
- given: Tom
  family: Goldstein
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/taylor16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
