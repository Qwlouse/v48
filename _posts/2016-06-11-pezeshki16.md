---
supplementary: Supplementary:pezeshki16-supp.pdf
title: Deconstructing the Ladder Network Architecture
abstract: The Ladder Network is a recent new approach to semi-supervised learning
  that turned out to be very successful. While showing impressive performance, the
  Ladder Network has many components intertwined, whose contributions are not obvious
  in such a complex architecture. This paper presents an extensive experimental investigation
  of variants of the Ladder Network in which we replaced or removed individual components
  to learn about their relative importance. For semi-supervised tasks, we conclude
  that the most important contribution is made by the lateral connections, followed
  by the application of noise, and the choice of what we refer to as the ‘combinator
  function’. As the number of labeled training examples increases, the lateral connections
  and the reconstruction criterion become less important, with most of the generalization
  improvement coming from the injection of noise in each layer. Finally, we introduce
  a combinator function that reduces test error rates on Permutation-Invariant MNIST
  to 0.57% for the supervised setting, and to 0.97% and 1.0% for semi-supervised settings
  with 1000 and 100 labeled examples, respectively.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: pezeshki16
month: 0
firstpage: 2368
lastpage: 2376
page: 2368-2376
sections: 
author:
- given: Mohammad
  family: Pezeshki
- given: Linxi
  family: Fan
- given: Philemon
  family: Brakel
- given: Aaron
  family: Courville
- given: Yoshua
  family: Bengio
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/pezeshki16/pezeshki16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
