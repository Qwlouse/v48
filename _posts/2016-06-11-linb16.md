---
title: Fixed Point Quantization of Deep Convolutional Networks
abstract: In recent years increasingly complex architectures for deep convolution
  networks (DCNs) have been proposed to boost the performance on image recognition
  tasks. However, the gains in performance have come at a cost of substantial increase
  in computation and model storage resources. Fixed point implementation of DCNs has
  the potential to alleviate some of these complexities and facilitate potential deployment
  on embedded hardware. In this paper, we propose a quantizer design for fixed point
  implementation of DCNs. We formulate and solve an optimization problem to identify
  optimal fixed point bit-width allocation across DCN layers. Our experiments show
  that in comparison to equal bit-width settings, the fixed point DCNs with optimized
  bit width allocation offer >20% reduction in the model size without any loss in
  accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further
  enhance the accuracy of fixed point DCNs beyond that of the original floating point
  model. In doing so, we report a new state-of-the-art fixed point performance of
  6.78% error-rate on CIFAR-10 benchmark.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: linb16
month: 0
tex_title: Fixed Point Quantization of Deep Convolutional Networks
firstpage: 2849
lastpage: 2858
page: 2849-2858
order: 2849
cycles: false
author:
- given: Darryl
  family: Lin
- given: Sachin
  family: Talathi
- given: Sreekanth
  family: Annapureddy
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/linb16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
