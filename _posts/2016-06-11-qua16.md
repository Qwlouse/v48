---
supplementary: Supplementary:qua16-supp.pdf
title: Fast Rate Analysis of Some Stochastic Optimization Algorithms
abstract: In this paper, we revisit three fundamental and popular stochastic optimization
  algorithms (namely, Online Proximal Gradient, Regularized Dual Averaging method
  and ADMM with online proximal gradient) and analyze their convergence speed under
  conditions weaker than those in literature. In particular, previous works showed
  that these algorithms converge at a rate of O (\ln T/T) when the loss function is
  strongly convex, and O (1 /\sqrtT) in the weakly convex case. In contrast, we relax
  the strong convexity assumption of the loss function, and show that the algorithms
  converge at a rate O (\ln T/T) if the \em expectation of the loss function is \em
  locally strongly convex. This is a much weaker assumption and is satisfied by many
  practical formulations including Lasso and Logistic Regression. Our analysis thus
  extends the applicability of these three methods, as well as provides a general
  recipe for improving analysis of convergence rate for stochastic and online optimization
  algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: qua16
month: 0
firstpage: 662
lastpage: 670
page: 662-670
sections: 
author:
- given: Chao
  family: Qu
- given: Huan
  family: Xu
- given: Chong
  family: Ong
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/qua16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
