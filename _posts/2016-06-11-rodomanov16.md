---
supplementary: Supplementary:rodomanov16-supp.pdf
title: A Superlinearly-Convergent Proximal Newton-type Method for the Optimization
  of Finite Sums
abstract: We consider the problem of minimizing the strongly convex sum of a finite
  number of convex functions. Standard algorithms for solving this problem in the
  class of incremental/stochastic methods have at most a linear convergence rate.
  We propose a new incremental method whose convergence rate is superlinear â€“ the
  Newton-type incremental method (NIM). The idea of the method is to introduce a model
  of the objective with the same sum-of-functions structure and further update a single
  component of the model per iteration. We prove that NIM has a superlinear local
  convergence rate and linear global convergence rate. Experiments show that the method
  is very effective for problems with a large number of functions and a small number
  of variables.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: rodomanov16
month: 0
tex_title: A Superlinearly-Convergent Proximal Newton-type Method for the Optimization
  of Finite Sums
firstpage: 2597
lastpage: 2605
page: 2597-2605
sections: 
author:
- given: Anton
  family: Rodomanov
- given: Dmitry
  family: Kropotov
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/rodomanov16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
