---
supplementary: Supplementary:perolat16-supp.pdf
title: Softened Approximate Policy Iteration for Markov Games
abstract: This paper reports theoretical and empirical investigations on the use of
  quasi-Newton methods to minimize the Optimal Bellman Residual (OBR) of zero-sum
  two-player Markov Games. First, it reveals that state-of-the-art algorithms can
  be derived by the direct application of Newton’s method to different norms of the
  OBR. More precisely, when applied to the norm of the OBR, Newton’s method results
  in the Bellman Residual Minimization Policy Iteration (BRMPI) and, when applied
  to the norm of the Projected OBR (POBR), it results into the standard Least Squares
  Policy Iteration (LSPI) algorithm. Consequently, new algorithms are proposed, making
  use of quasi-Newton methods to minimize the OBR and the POBR so as to take benefit
  of enhanced empirical performances at low cost. Indeed, using a quasi-Newton method
  approach introduces slight modifications in term of coding of LSPI and BRMPI but
  improves significantly both the stability and the performance of those algorithms.
  These phenomena are illustrated on an experiment conducted on artificially constructed
  games called Garnets.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: perolat16
month: 0
firstpage: 1860
lastpage: 1868
page: 1860-1868
sections: 
author:
- given: Julien
  family: Pérolat
- given: Bilal
  family: Piot
- given: Matthieu
  family: Geist
- given: Bruno
  family: Scherrer
- given: Olivier
  family: Pietquin
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/perolat16/perolat16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
