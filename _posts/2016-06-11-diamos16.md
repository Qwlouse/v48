---
title: 'Persistent RNNs: Stashing Recurrent Weights On-Chip'
abstract: This paper introduces a new technique for mapping Deep Recurrent Neural
  Networks (RNN) efficiently onto GPUs. We show how it is possi- ble to achieve substantially
  higher computational throughput at low mini-batch sizes than direct implementations
  of RNNs based on matrix multiplications. The key to our approach is the use of persistent
  computational kernels that exploit the GPUâ€™s inverted memory hierarchy to reuse
  network weights over multiple timesteps. Our initial implementation sustains 2.8
  TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU. This provides a 16x reduction
  in activation memory footprint, enables model training with 12x more parameters
  on the same hardware, allows us to strongly scale RNN training to 128 GPUs, and
  allows us to efficiently explore end-to-end speech recognition models with over
  100 layers.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: diamos16
month: 0
firstpage: 2024
lastpage: 2033
page: 2024-2033
sections: 
author:
- given: Greg
  family: Diamos
- given: Shubho
  family: Sengupta
- given: Bryan
  family: Catanzaro
- given: Mike
  family: Chrzanowski
- given: Adam
  family: Coates
- given: Erich
  family: Elsen
- given: Jesse
  family: Engel
- given: Awni
  family: Hannun
- given: Sanjeev
  family: Satheesh
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/diamos16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
