---
supplementary: Supplementary:hazanb16-supp.pdf
title: On Graduated Optimization for Stochastic Non-Convex Problems
abstract: The graduated optimization approach, also known as the continuation method,
  is a popular heuristic to solving non-convex problems that has received renewed
  interest over the last decade.Despite being popular, very little is known in terms
  of its theoretical convergence analysis. In this paper we describe a new first-order
  algorithm based on graduated optimization and analyze its performance. We characterize
  a family of non-convex functions for which this algorithm provably converges to
  a global optimum. In particular, we prove that the algorithm converges to an ε-approximate
  solution within O(1 / ε^2) gradient-based steps. We extend our algorithm and analysis
  to the setting of stochastic non-convex optimization with noisy gradient feedback,
  attaining the same convergence rate. Additionally, we discuss the setting of “zero-order
  optimization", and devise a variant of our algorithm which converges at rate of
  O(d^2/ ε^4).
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hazanb16
month: 0
firstpage: 1833
lastpage: 1841
page: 1833-1841
sections: 
author:
- given: Elad
  family: Hazan
- given: Kfir Yehuda
  family: Levy
- given: Shai
  family: Shalev-Shwartz
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/hazanb16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
