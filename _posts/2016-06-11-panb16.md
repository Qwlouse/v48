---
supplementary: Supplementary:panb16-supp.pdf
title: Expressiveness of Rectifier Networks
abstract: Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing
  gradient problem, allow for efficient backpropagation, and empirically promote sparsity
  in the learned parameters. They have led to state-of-the-art results in a variety
  of applications. However, unlike threshold and sigmoid networks, ReLU networks are
  less explored from the perspective of their expressiveness. This paper studies the
  expressiveness of ReLU networks. We characterize the decision boundary of two-layer
  ReLU networks by constructing functionally equivalent threshold networks. We show
  that while the decision boundary of a two-layer ReLU network can be captured by
  a threshold network, the latter may require an exponentially larger number of hidden
  units. We also formulate sufficient conditions for a corresponding logarithmic reduction
  in the number of hidden units to represent a sign network as a ReLU network. Finally,
  we experimentally compare threshold networks and their much smaller ReLU counterparts
  with respect to their ability to learn from synthetically generated data.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: panb16
month: 0
firstpage: 2427
lastpage: 2435
page: 2427-2435
sections: 
author:
- given: Xingyuan
  family: Pan
- given: Vivek
  family: Srikumar
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/panb16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
