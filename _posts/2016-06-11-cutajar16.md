---
supplementary: Supplementary:cutajar16-supp.pdf
title: Preconditioning Kernel Matrices
abstract: 'The computational and storage complexity of kernel machines presents the
  primary barrier to their scaling to large, modern, datasets. A common way to tackle
  the scalability issue is to use the conjugate gradient algorithm, which relieves
  the constraints on both storage (the kernel matrix need not be stored) and computation
  (both stochastic gradients and parallelization can be used). Even so, conjugate
  gradient is not without its own issues: the conditioning of kernel matrices is often
  such that conjugate gradients will have poor convergence in practice. Preconditioning
  is a common approach to alleviating this issue. Here we propose preconditioned conjugate
  gradients for kernel machines, and develop a broad range of preconditioners particularly
  useful for kernel matrices. We describe a scalable approach to both solving kernel
  machines and learning their hyperparameters. We show this approach is exact in the
  limit of iterations and outperforms state-of-the-art approximations for a given
  computational budget.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cutajar16
month: 0
tex_title: Preconditioning Kernel Matrices
firstpage: 2529
lastpage: 2538
page: 2529-2538
sections: 
author:
- given: Kurt
  family: Cutajar
- given: Michael
  family: Osborne
- given: John
  family: Cunningham
- given: Maurizio
  family: Filippone
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/cutajar16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
