---
supplementary: Supplementary:pedregosa16-supp.zip
title: Hyperparameter optimization with approximate gradient
abstract: Most models in machine learning contain at least one hyperparameter to control
  for model complexity. Choosing an appropriate set of hyperparameters is both crucial
  in terms of model accuracy and computationally challenging. In this work we propose
  an algorithm for the optimization of continuous hyperparameters using inexact gradient
  information. An advantage of this method is that hyperparameters can be updated
  before model parameters have fully converged. We also give sufficient conditions
  for the global convergence of this method, based on regularity conditions of the
  involved functions and summability of errors. Finally, we validate the empirical
  performance of this method on the estimation of regularization constants of L2-regularized
  logistic regression and kernel Ridge regression. Empirical benchmarks indicate that
  our approach is highly competitive with respect to state of the art methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: pedregosa16
month: 0
firstpage: 737
lastpage: 746
page: 737-746
sections: 
author:
- given: Fabian
  family: Pedregosa
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/pedregosa16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
