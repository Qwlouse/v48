<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks | ICML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks">

  <meta name="citation_author" content="Arpit, Devansh">

  <meta name="citation_author" content="Zhou, Yingbo">

  <meta name="citation_author" content="Kota, Bhargava">

  <meta name="citation_author" content="Govindaraju, Venu">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 33rd International Conference on Machine Learning">
<meta name="citation_firstpage" content="1168">
<meta name="citation_lastpage" content="1176">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v48/arpitb16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks</h1>

	<div id="authors">
	
		Devansh Arpit,
	
		Yingbo Zhou,
	
		Bhargava Kota,
	
		Venu Govindaraju
	<br />
	</div>
	<div id="info">
		Proceedings of The 33rd International Conference on Machine Learning,
		pp. 1168–1176, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks– <em>Internal Covariate Shift</em>– the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size <span class="math">\( 1 \)</span> during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call <em>Normalization Propagation</em>. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="arpitb16.pdf">Download PDF</a></li>
			
			<li><a href="arpitb16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
